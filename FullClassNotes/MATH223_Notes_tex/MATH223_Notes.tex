\documentclass[
12pt,
]{article}
\usepackage{titlesec}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{physics}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{undertilde}
\usepackage{dutchcal}
\usepackage{amsthm}
\usepackage{xfrac}
\newcommand{\tx}{\text{}}
\usepackage{tikz}
\newcommand{\td}{\text{dim}}
\newcommand{\tvw}{T : V\xrightarrow{} W }
\newcommand{\ttt}{\widetilde{T}}
\newcommand{\ex}{\textbf{Example}}
\newcommand{\aR}{\alpha \in \mathbb{R}}
\newcommand{\abR}{\alpha \beta \in \mathbb{R}}
\newcommand{\un}{u_1 , u_2 , \dots , n}
\newcommand{\an}{\alpha_1, \alpha_2, \dots, \alpha_2 }
\newcommand{\sS}{\text{Span}(\mathcal{S})}
\newcommand{\sSt}{($\mathcal{S}$)}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\usepackage{mathtools}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\newcommand{\vectorproj}[2][]{\textit{proj}_{\vect{#1}}\vect{#2}}
\newcommand{\vect}{\mathbf}


\newtheorem{theorem}{Theorem}[section]
\theoremstyle{definition}
\newtheorem{corollary}{Corollary}[theorem]
\theoremstyle{definition}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{Definition}{Definition}[section]
\theoremstyle{definition}
\newtheorem{Proposition}{Proposition}[section]

\title{Notes For MATH223}
\titleformat*{\section}{ \large\normalfont\fontsize{12}{12}\bfseries}
\titleformat*{\subsection}{ \large\normalfont\fontsize{10}{15}\bfseries}
\author{Mihail Anghelici 260928404}
\date{\empty}

\begin{document}
\maketitle
\section{Vector Spaces}
\textbf{Propreties of Vectors Spaces}
\begin{align*}
	&\textbf{1. } \text{The element $0_v$ of A4 is unique ,it's the zero vector} \\ 
	&\textbf{2. } \forall u \in V \quad \text{the vector $-u$ from A5 is also unique.} \\
	&\textbf{3. } \alpha 0_v = 0_v  \\
	&\textbf{4. } 0u = 0_v \qquad \forall \ u \in V  \\
	&\textbf{5. } \forall u, v, w \in V \qquad u+v =u+w \implies v=w \\
	&\textbf{6. } \alpha \in \mathbb{R} , u \ \in V \\
	&\hspace{10pt} \alpha u = 0_v \iff (\alpha=0) \ \text{or} \ (u=0_v)
\end{align*}
\subsection{Subspaces}
Let $(V, + , \cdot )$ be a vector space and let $W$ be a subset of $V$.
\begin{Definition}
	W is said to be a subspace of $V$ if $W$ equipped with the two operations is a vector space. 
\end{Definition}
\textbf{Remark} We do not need to verify all 10 axioms on $W$ . It is enough to verify that 
\begin{enumerate}
	\item W is not empty  
	\item W is closed under addition i.e whenever $u,v \in W, u+v \in W$ 
	\item W is closed under scalar multiplication \newline i.e whenever $u \in W , \alpha \in \mathbb{R}, \alpha u \in W$
\end{enumerate}
\begin{Proposition}
	Let $V$ be a vector space and let $W$ be a non-empty subset of $V$. $W$ is a subpace of $V$ iff whenever $u,v \in W \alpha , \beta \in \mathbb{R} , \alpha u + \beta v \in W $
\end{Proposition}
\textbf{Example}\\
Let $\mathcal{P}_n$ be the set of all polynominals defined on $\mathbb{R}$ with degree $\leq n$. $\mathcal{P}_n$ is a subspace of $\mathbb{F}(\mathbb{R})$. \\
Let $n = 2$.
\begin{align*}
	\text{take } f(x) &= x^2 \\	
	g(x) &= -x^2 +1 \\(f+g)(x) 
	&= 1 \quad \xrightarrow{} \text{deg}(0)
 \end{align*}
So it's not closed under addition nor scalar multiplication. \\ \\ 
\textbf{Example}\\
Let $V = \{M \in \mathscr{M}_{n\cross n} | \text{ such that} \quad A=A^{T} \xrightarrow{} \text{set of all  symmetric matrices } \}$\\
\begin{align*}
	&\textbf{i. } \utilde{O} \in V \qquad \text{Verify emptyness} \\
	&\textbf{ii, } \text{Let } A,B \in V \quad , (A+B)^{T} = A^T + B^T = A + B \quad \\
	&\textbf{iii, } \text{Let } A \in V , \ \alpha \in \mathbb{R} \qquad , \alpha A^T = \alpha A
\end{align*}
So $V$ as defined is a vector space of $\mathscr{M}_{n\cross n}$.
\section{Spanning Sets}
\begin{Definition}{Linear Combination}
	Let $V$ be a vector space. \\
	Let $ \un $ be vectors in $V$. \\
	A linear combination of $\un $ is any vector $u\in V$ of the form \\
	\begin{gather*}
		 u = \sum_{i = 1}^{n} \alpha_i u_i \\
		 \implies \alpha_1 u_1 + \alpha_2 u_2 + \dots + \alpha_n u_n \qquad \text{where  } \an \in \mathbb{R}
	\end{gather*}
\end{Definition}
\begin{Definition}{Spanning Set}\\
	Let $S$ be a non empty subset of a vector space $V$. Span$\{S\}$ is the subset of $V$ defined as Span($\mathcal{S}$).
	\begin{gather*}
		\sS = \{u = \sum_{i=1}^{n} \alpha_i u_i | n \in \mathbb{N}, \\
		 \un \in \mathcal{S}, \quad  \an \in \mathbb{R} \}
	\end{gather*}
\end{Definition}
\textbf{Remark} $\mathcal{S} \subseteq \sS $
\begin{Proposition}
	Let $V$ be a vector space and $\mathcal{S}$ be a non empty subset of $V$. Then , Span\sSt is the smallest vector subspace of $V$ which contains $\mathcal{S}$
\end{Proposition} 
\textbf{Propreties of spanning sets}\\
\begin{enumerate}
	\item $ \mathcal{S} = \sS $ if an only if $\mathcal{S}$ is a subspace \\
	\item Span(\sSt) = \sSt        same span of span of span...
\end{enumerate}
\ex \\
 $\mathcal{P}_2 $ as a subspace of $\mathbb{F} (\mathbb{R})$ \\
Let $f \in \mathcal{P}_2 $ \quad , $\exists$ \ $a,b,c \in \mathbb{R}$ such that $f(x) = ax^2 +bx +c $ \\ \\
\textbf{Exercise}
Let $W$ = $\{p \in \mathcal{P}_3 | p(1) = 0\}$\\
Prove that $W$ is a subspace of $\mathcal{P}_3$ \\
\begin{gather*}
	p(x) = (x-1)[ax^2 + bx + c] \\
	W =  \text{Span} \{x-1 , x(x-1), x^2 (x-1)\}
	\intertext{That's how a cubic polynominal vanishes at x =1}
\end{gather*} 
\ex 
\ Let us find a spanning set $\mathscr{M}_{(2\cross 2)}$. 
\begin{gather*}
	M \in \mathscr{M}_{(2\cross 2)}, M = 
	\begin{bmatrix}
		a_{11} & a_{12} \\
		a_{21} & a_{22}
	\end{bmatrix} 
	= a_{11}
	\begin{bmatrix}
		1 & 0 \\
		0 & 0
	\end{bmatrix} 
	= a_{12}
	\begin{bmatrix}
		0 & 1 \\
		0 & 0
	\end{bmatrix} 
	= a_{21}
	\begin{bmatrix}
		0 & 0 \\
		1 & 0
	\end{bmatrix} 
	= a_{22}
	\begin{bmatrix}
		0 & 0 \\
		0 & 1
	\end{bmatrix} \\ 
	\intertext{Let } 
	E_1
	\begin{bmatrix}
		1 & 0 \\
		0 & 0
	\end{bmatrix} ,
	E_2
	\begin{bmatrix}
		0 & 1 \\
		0 & 0
	\end{bmatrix}, 
	E_3
	\begin{bmatrix}
		0 & 0 \\
		1 & 0
	\end{bmatrix}, 
	E_4
	\begin{bmatrix}
		0 & 0 \\
		0 & 1
	\end{bmatrix}
	\intertext{$\mathscr{M}_{(2\cross 2)} = Span\{ E_1, E_2, E_3, E_4 \}$}	
\end{gather*}
\textbf{Note} \\
\begin{enumerate}
	\item For a $(n\cross n)$ matrix we have a spanning set of $(m\cross n)$ matrices. 
	\item A spanning set is not unique, we can come up with an infinite number of them. 
\end{enumerate}
\section{Linearly Independent Subsets}
Let \sSt be a non-empty subset of a vector space $V$. \\
\begin{Definition}
	$\mathcal{S} = \{\un\}$ is said to be linearly independent (LI) iff whenever 
	\begin{gather*}
		\sum_{i = 1}^{n} \alpha_i u_i = 0 \quad \text{where} \quad \alpha_1 = \alpha_2 = \alpha_3 =0
	\end{gather*}
\end{Definition}
\textbf{Remark} \\
If $\mathcal{S} = \{\un\}$ is not linearly independent we say that $\mathcal{S}$ is linearly dependent. \\
\textbf{Propreties of Linear Independce} \\
$V$ is a vector space 
\begin{enumerate}
	\item $\{0_V\}$ is linearly dependent (rank = 0) \\
	Moreover any subsets of $\mathcal{S}$ of $V$ which contains $0_V$ is linearly dependent. 
	\item $\{u_n\}$ is linearly independent iff $ u\neq 0_V$ 
	\item Let $\mathcal{S} = \{\un\}$ be linearly independent. \\
	Let $u \in V$. $\mathcal{S} \cup \{u\} = \{\un , u\}$
\end{enumerate}
\section{Basis and Dimension}
\begin{lemma}
Let $V$ be a vector space and $\mathcal{S} = \{\un\}$ be a spanning set of $V$ and $\mathcal{B} = \{v_1 , v_2 , \dots , v_m\}$ a linearly independent set of vectors in $V$, then 
\begin{center}
 \fbox{$m \leq n$}
\end{center}
\end{lemma}
\begin{Definition}{Exchange Lemma}
	In the setting above, we can replace $m$ vectors $u_i$ in the subset $\mathcal{S}$ by the $v_i$ (all of them) and the resulting set will still be a spanning set of $V$.
\end{Definition}
\begin{Definition}
	Let $V$ be a vector space. A subset $\mathcal{B} = \un$ is called a basis of $V$ if $\mathcal{B}$ is both a spanning set of $V$ and linearly independent. 
\end{Definition}
\begin{Proposition}
	Let $V$ be a vector space. Then all bases of $V$ have the same number of vectors, called dimension of $V$ and denoted $\dim V$.
\end{Proposition}
\ex \\
Let $A_{(n\cross m)}$ , \quad Null($A$) = $ \{x\in \mathcal{B}^m \quad \text{such that} \quad AX = \utilde{O}\}$ \\
Then Null($A$) is a subspace of $\mathcal{B}^m$ \\
and $\dim(\text{Null}(A)) = m -\rank (A)$ \\
\textbf{Remark} \\
Dim($V$) where $V=\{0_V\}$ is 0. \\
\begin{Proposition}
	Let $V$ be a finite dimensional vector space and $E,F$ be two subspaces of $V$.
	\begin{enumerate}
		\item If $E \subseteq F \xrightarrow{} \dim(E) \leq \dim(F)$
		\item If $E \subseteq F$ and the $\dim(E) = \dim(F)$ then $E=F$
		\item If $\dim(E) = k$ then 
		\begin{enumerate}
			\item Every spanning set of $E$ has at least $k$ elements 
			\item Every linearly independent subsets of $E$ has at most $k$ elements.
			\item Every spanning set of $E$ which has exactly $k$ elements is a basis of $E$.
			\item Every linearly independent subsets of $E$ which contains exactly $k$ elements is a basis of $E$.
		\end{enumerate}
	\end{enumerate}
\end{Proposition} 
\textbf{Remark} \\
If you know the dimension of a subspace you can then know a lot of things , it's a critical information !
\section{Sum and Direct Sum of Subspaces} 
\begin{Definition}
	Let $E$ and $F$ be 2 subspaces of a vector space. The sum of $E$ and $F$ , $E + F$ is the subset defined as 
	\begin{gather*}
		E + F = \{u+v \quad \text{where } u\in E, v\in F\}
	\end{gather*}
\end{Definition}
\begin{Proposition}
	$E+F$ is a vector subspace of $V$.
\end{Proposition}
\begin{Definition}
	IF $F$ and $E$ are 2 subspaces of $V$ such that $ E \cap F = \{0_V\}$ then $E+F$ is called the direct sum of $E$ and $F$ and is denoted $E \oplus F$
\end{Definition}
\begin{Proposition}
	Let $E$ and $F$ be 2 subspaces of a vector space $V$. If $E \cap F = \{ 0_V \}$ , then every vector $w$ in $E \oplus F$ can be written uniquely as $w = u +v$ , where $u \in E$ and $v \in F$
\end{Proposition}
\begin{Proposition}
	Let $S = \{\un\}$ and $F = \{v_1, v_2 ,\dots, v_m\}$ be subsets of a vector space $V$. \\ 
	Define $E = \text{span} \{S\}$ and $F = \text{Span} \{F\}$ then ,\\
	\begin{enumerate}
		\item $S \cup F$ is a spanning set of $E + F$ 
		\item If $S$ and $F$ are linearly independent , then \\
		$S \cup F$ is linearly independent iff $E \cap F = \{0_V\}$
	\end{enumerate}
\end{Proposition}
\begin{corollary}
	If $E$ and $F$ are 2 finite dimensional subspaces of $V$ and $S = \un$,  $ F = \{v_1, v_2, \dots ,v_m\}$ are the respective bases of $E$ and $F$ \\ 
	If $E \cap F = \{0_V\}$ then $S \cup F$ is a basis of $E \oplus F$ i.e : 
	\begin{gather*}
		\dim(E \oplus F) = \dim(E) + \dim(F)
	\end{gather*}
\end{corollary}
\begin{lemma}
	Let $V$ be a vector space (finite dimensional) and $E$ be a subspace of $V$. There exists a subspace $E_1$ of $V$ such that $V = E \oplus E_1$. 
\end{lemma}
\begin{Proposition}
	If $E$ and $F$ are finite dimensional subspaces of a vector space $V$ , then 
	\begin{gather*}
		\dim(E+F) = \dim(E) + \dim(F) - \dim(E \cap F)
	\end{gather*}
\end{Proposition}
\ex \\
$V = \mathcal{P}_3$ 
\begin{align*}
	& E = \{p \in \mathcal{P}_3 | p^\prime (1) = 0 \} \xrightarrow{} \dim = 3 \\
	& F = \{p \in \mathcal{P}_3 | p (0) = 0 \} \xrightarrow{} \dim = 3 
\end{align*}
\begin{gather*}
	\text{True or false? } V = E + F \\
	\text{Let us find the dimension of } E \cap F. \\
	\text{Let } p \in E \cap F \\
	 \text{then} \quad p \in F \quad \text{i.e} \quad p(x) = ax^3 + bx^2 +cx  \\
	 \text{also} \quad p \in E \quad \text{i.e} \quad p^\prime (1) = 3a + 2b + c = 0\\
	  \hspace{10pt} \implies c = -3a -2b \\
	  p(x) = a(x^3 -3x) + b(x^2 -2x) \qquad , a,b \in \mathbb{R}
	 \intertext{Therefore } \quad E \cap F = \text{Span} \{x^3 -3x, x^2 -2x\} \\
	  \text{So} \quad \{x^3 -3x , x^2 -2x\} \ \text{is L.I, therefore a basis of } E \cap F
\end{gather*}
Since $\dim(E \cap F ) =2$, \quad thus $\dim(E + F) = 3 + 3 -2 = 4$.
\section{Coordinates of a Vector Relative to a Basis}
\begin{Proposition}
	Let $V$ be a vector space such that $\dim(V) = n$ \\
	Let $\mathcal{B} = \un $ be a basis of $V$. \\
	For every vector $u \in V$ , there exists a unique $(n \cross 1)$ matrix $\utilde{X} = 
	\begin{bmatrix}
		x_1 \\
		x_2 \\
		\vdots \\
		x_n
	\end{bmatrix}$ 
	Such that , $u = \sum_{i=1}^{n} x_i u_i$
\end{Proposition}
\begin{Definition}
	Given $\dim(V) = n$ and $\mathcal{B} = \un$ a basis of $V$ , for $u\in V$ ,the unique vector \\
	 $\utilde{X} = 
		\begin{bmatrix}
			x_1 \\
			x_2 \\
			\vdots \\
			x_n
		\end{bmatrix} \in \mathbb{R}^n $ \qquad such that , $u = \sum_{i=1}^{n} x_i u_i$ ,
		is called the coordinate vector of $u$ relative to $\mathcal{B}$ and denoted $\underbrace{[u]_{\mathcal{B}}}_{n\cross 1}$
\end{Definition}
\begin{Proposition}
	\begin{align*}
		\text{Let } \quad & \mathcal{B} = \{\un \} \\
		& \mathcal{S} = \{v_1, v_2, \dots, v_n\} \quad \text{2 bases of same vector space}
	\end{align*}
	There exists a unique matrix $(n\cross n)$ denoted $\mathcal{P}_{\mathcal{B}, \mathcal{S}}$ and called the transition matrix from $\mathcal{B}$ to $\mathcal{S}$ such that $\forall u \in V$ 
	\begin{gather*}
		\boxed{[w]_{\mathcal{S}} = \mathcal{P}_{\mathcal{B}, \mathcal{S}} [w]_{\mathcal{B}}}
	\end{gather*}
\end{Proposition}
\begin{corollary}
	Given $\mathcal{S}$ and $\mathcal{B}$ , 
	\begin{center}
		$\mathcal{P}_{\mathcal{B}, \mathcal{S}}\mathcal{P}_{\mathcal{S}, \mathcal{B}} = I_n$
	\end{center}
\end{corollary}
\section{Linear Transformations}
\begin{Definition}
	Let $V$ and $W$ be two vector spaces \\
	A function $\tvw$ (from $V$ to $W$) is called a linear transformation iff whenever $u_1, u_2 \in V$ and $\alpha_1, \alpha_2 \in \mathbb{R}$ , 
	\begin{gather*}
		T(\alpha_1 u_1 + \alpha_2 u_2) = \alpha_1 T (v_1) + \alpha_2 T(v_2)
	\end{gather*}
\end{Definition}
\textbf{Propreties of Linear Transformations} \\
\begin{enumerate}
	\item If $\tvw$ is a linear transformation then $T(0_V) = 0_W$ 
	\item If $T_1$ and $T_2$ are two linear transformations from $V$ into $W$ and $\alpha_1 , \alpha_2 \in \mathbb{R}$ ,then 
	\begin{align*}
		(\alpha_1 T_1 + \alpha_2 T_2)(v) = \alpha_1 T_1 (v) + \alpha_2 T_2 (v)
	\intertext{Then this is a linear transformation from $V$ into $W$}
	\end{align*}
\end{enumerate}
\begin{Definition}
	Given $\tvw$ a linear transformation, the set $\{u\in V | T(u) = 0_W\}$ is a vector subspace of $V$ called the Kernel of $T$ and is denoted $\ker(T)$ 
	\begin{align*}
		\text{Ker}(T) = & \{u\in V | T(u) = 0_W\} \\
		& = T^{-1} (\{0_W\})
	\end{align*}
\end{Definition}
\textbf{Picture} \\
\begin{tikzpicture}
\draw (8,2) ellipse (1cm and 2cm);
\draw (3,2) ellipse (1cm and 2cm);
\draw[red,thick,dashed] (8,2) ellipse (0.75cm and 1.25cm);
\draw[red,thick,dashed] (3,2) ellipse (0.75cm and 1.25cm);
\node [label={[xshift=3.0cm, yshift=4cm]$V$}];
\node [label={[xshift=8.0cm, yshift=4cm]$W$}];
\node [label={[xshift=3.0cm, yshift=3.1cm]$\ker(T)$}];
\node [label={[xshift=8.0cm, yshift=3.1cm]$\Im(T)$}];
\draw[->] (8,2.5) -- (3,2.5);
\end{tikzpicture}
\begin{Definition}{One-to-One}
	$\tvw$ a linear transformation is said to be one-to-one (or injective) if whenever 
	\begin{gather*}
		T(u_1) = T(u_2) \implies u_1 = u_2
	\end{gather*}
	i.e , we can't have same vector with different output.
\end{Definition}
\begin{Proposition}
	$\tvw$ , a linear transformation is one-to-one iff 
	\begin{gather*}
		\ker(T) = \{0_V\}
	\end{gather*}
	i.e , only one element mapped to $0_W$ is $0_V$.
\end{Proposition}
\begin{Proposition}
	Let $\tvw$ be a one-to-one transformation \\
	Let $\mathcal{B} = \{\un \}$ be a linearly independent subset of $V$ ,then \\
	\begin{gather*}
		T(\mathcal{B}) = \{ T(u_1), T(u_2), \dots, T(u_n) \} \quad \text{is L.I as well}
	\end{gather*}
	So a one-to-one transformation carries linear independence in $W$
\end{Proposition}
\begin{Definition}{Image of a Linear Transformation}
	$\tvw$ is a linear transformation. The image of $T$ denoted $\Im(T)$ is the range of $T$ (a subset of $W$). 
	\begin{gather*}
		\Im(T) = \{w\in W | w = T(u) \quad \text{for some } u \in V\}
	\end{gather*}
\end{Definition}
\begin{Proposition}
	$\tvw$ a linear transformation, $\Im(T)$ is a vector subspace of $W$.
\end{Proposition}
\begin{Definition}{Image of Linear Transformations}
	$\tvw$ a linear transformation then,
	\begin{gather*}
		\Im(T) = \{ w \in W | \exists u \in V \quad \text{and} \quad w =T(u)\}
	\end{gather*}
	$\Im(T)$ is the range of $T$.
\end{Definition}
\begin{Definition}
	$\tvw$ , a linear transformation, is said to be onto (surjective) if 
	\begin{gather*}
		\Im(T) = W
	\end{gather*}
\end{Definition}
\textbf{Propreties of surjective trasformations} \\
Let $\tvw$ be a linear transformation and $\mathcal{S} = \{\un\}$ a spanning set of $V$ then $\{T(u_1), \dots , T(u_n)\}$ is a spanning set of $\Im(T)$\\
\textbf{Note} If we're asked to find a basis of $\Im(T)$ the previous proprety is very usefull. \\




\begin{Definition} .A linear trnasformation $T:V \xrightarrow{} W$ is said to be an \textbf{isomorphism} if $T$ is one-to-one and onto.\\
\end{Definition}
\begin{Proposition} .Let $T:V \xrightarrow{} W$ be an isomorphism. Assume that dim(V) is finite.\\
\quad\quad Then,
$$ \text{dim}(V) = \text{dim}(W)$$
\end{Proposition}
Indeed since T is defined to be an isomorphism, its range has to be W itself, therefore the number of vectors that span each subspaces is the same and consequently dimension is preserved.\\
Under these circumstances, we say that isomorphism carries not only span, but the basis as well.
\section{Composition of Linear Transformation}
Let $V1,V2,V3 $ and $T_1, T_2$ be defined as:
\begin{align*}
    &T_1 : V_1 \xrightarrow{} V_2 \\
    &T_2 : V_2 \xrightarrow{} V_3 
\end{align*}
 \large$$ V_1 \xrightarrow{T_1} V_2 \xrightarrow{T_2} V_3$$
\normalsize
Where $T_i$ are linear transformations. 
\begin{Proposition}
The function $T_2 \circ T_1 : V_1 \xrightarrow{} V_3$ is a linear transformation. 
\end{Proposition}
\begin{Proposition}Let $\tvw$ \text{be an isomorphism. The inverse of } $T_1$ denoted $T^{-1} : W \xrightarrow{} V$ is also a linear transformation. 
\end{Proposition} \\
\textbf{Note} By definition of $T^{-1}$, given $w \in W $ , $T^{-1}(w)$ is the unique vector in $V$ such that $T(T^{-1}(w))=w$. \\
\\
\textbf{Remark} 
\begin{align*}
    &1) T\circ T^{-1} = Id_W\quad\quad \text{Identity L.T of W} \\
    &2) T^{-1} \circ T = Id_V   
\end{align*}
 \large$$ V \xrightarrow{T} W \xrightarrow{T^{-1}} V$$
\normalsize
\quad\quad Where $Id_W$ and $Id_V$ are the identity transformations on $V$ and $W$ respectively. 
\begin{align*}
     &Id_V : V \xrightarrow{} W \\
     &Id_V(u) = u \quad\quad \forall u \in V
\end{align*}
\begin{theorem}{Rank Nullity Theorem}
\normalfont $\tvw$ is a linear transformation and $ \dim(V)$ is finite,
\end{theorem}
\begin{center}
    \fbox{dim(V) = dim(Ker(T)) + dim(Im(T))}
\end{center}
\begin{Definition}
$T : R^{n}\xrightarrow{} R^{n} $ What makes this linear transformation? \newline
then there $\exists $ a unique standard matrix of $T$ 
\end{Definition}
For example we can take 
\begin{align*}
    T
    \begin{bmatrix}
    x \\
    y \\
    z 
    \end{bmatrix}=
    \begin{bmatrix}
    3y + 2x \\
    z - 3y \\
    x + y 
    \end{bmatrix}
    \longmapsto 
    \quad\quad[T]=
    \begin{bmatrix}
    2 & 3 & 0 \\
    0 & -3 & 1 \\
    1 & 1 & 0
    \end{bmatrix}
\end{align*}
\begin{Definition}
$\ttt$ = $\mathcal{i}_\mathcal{B} \circ \mathcal{i}_\mathcal{B}^{-1} $\\
$\ttt$ is a linear transformation from $R^{n}\xrightarrow{} R^{n} $ so there $\exists$ a unique matrix $(n\times n)$ [$\ttt$] such that $\ttt(X)= [\ttt]X$ \quad $\forall \ X \in \mathbb{R}^{n}, \forall u \in V $
\end{Definition}
\begin{center}
    $\boxed{[T(u)]_{\mathcal{B}} = [\ttt][u]_\mathcal{B}}$
\end{center}
\begin{Proposition}
    If $ \tvw $ is  a linear transformation and $\mathcal{B} = \{u_1,...,u_n\}$ is a basis of V, then $\exists$ a unique matrix $(n\cross n)$ matrix denoted $[T]_\mathcal{B}$ called the representation of $T$ relative to $\mathcal{B}$ such that \\
    \begin{center}
        $[T(u)]_\mathcal{B} = [T]_\mathcal{B}[u]_\mathcal{B}$ \quad $\forall u \in V$     
    \end{center}
\end{Proposition}
\textbf{Remark}
$[T]_\mathcal{B}$ is the standard matrix of $\ttt$ where $\ttt$ =  $\mathcal{i}_\mathcal{B} \circ T \circ \mathcal{i}_\mathcal{B}^{-1}$ \\ How to find $[T]_\mathcal{B}$ in practice ? 
\begin{align*}
    \text{set } u=u_i \text{ in the result of proposition 1.3} \\
    [u_i]_\mathcal{B} = 
    \begin{bmatrix}
    0 \\
    0 \\
    1 \\
    0 \\
    0
    \end{bmatrix}
    \quad \xleftarrow  \quad \text{ith position}\\
    [T]_\mathcal{B}[u_i]_\mathcal{B} = \text{ ith column of } [T(u_i)]_\mathcal{B}
\end{align*}
\section{Matrix Representation of Linear Transformations}
$\tvw $ Assume dim(V) = n and $\mathcal{B} = \{u_1,..,u_n \}$ a basis of V.\\
then we get the following result ; $\exists$ a unique matrix $(n\cross n)$ denoted $(\exists !)$ of T , relative to $\mathcal{B}$. Such that $\forall u$ \\ \\
\textbf{Remark} The ith column of $[T]_\mathcal{B}$ is equal to $[T(u_i)]_\mathcal{B}$ where $u_i$ is the ith vector of the (ordered) basis $\mathcal{B}$\\ \\
\textbf{Picture: }
\begin{align*}
    &\quad\quad V \text{  }\xrightarrow{T} V\\
    &\mathcal{i}_\mathcal{B}^{-1}\big\uparrow\quad\quad\quad\quad \big\downarrow\mathcal{i}_\mathcal{B} \\
    &\quad\quad \mathbb{R}^{n} \xrightarrow{\widetilde{T}} \mathbb{R}^{n}
\end{align*}
\begin{Definition}{Similar Matrices.}
Two $(n\cross n)$ matrices $A$ and $B$ are said to be similar if $\exists$ a $(n\cross n)$ invertible matrix $P$ such that,
\begin{center}
    $A=PBP^{-1}$
\end{center}
\end{Definition}
Note that diagonalizable matrices are similar to diagonal matrices.
\begin{Proposition}
    Assume dim(V) = n , and $\mathcal{B}$ and $\mathcal{S}$ are two bases of V.\\
    If $T : V \xrightarrow{} V$ is a linear transformation then, $[T]_\mathcal{B}$ and $[T]_\mathcal{S}$ are similar. More precisely, 
    \begin{center}
        $[T]_\mathcal{S} = P_{\mathcal{B},\mathcal{S}}[T]_\mathcal{B}P_{\mathcal{B},\mathcal{S}}^{-1}$ 
    \end{center}
\end{Proposition}
\textbf{Remark} 
$T : V \xrightarrow{} V $ , dim(V) = n , Question: is there a basis $\mathcal{S}$ of V such that $[T]_\mathcal{S}$ is diagonal ? \\
Starting with a basis $\mathcal{B}$ of V, if such a basis exists then
\begin{center}
    $[T]_\mathcal{B} = P_{\mathcal{S},\mathcal{B}}\underbrace{[T]_\mathcal{S}}_\text{diagonal}P_{\mathcal{S},\mathcal{B}}^{-1}$ 
\end{center}
(i.e) $[T]_\mathcal{B}$ is diagonalizable but not necessarily diagonal.\\
In summary, such a basis $\mathcal{S}$ exists iff $[T]_\mathcal{B}$ is diagonalizable for any other basis $\mathcal{B}$.\\
Note that the ith column of $P$ is an eigenvector coresponding to $ \lambda_i$\\
\qquad From this reasoning we conclude that if $P_{\mathcal{B},\mathcal{S}} = P$, then $[T]_\mathcal{S}=D$ , moreover $P_{\mathcal{S},\mathcal{B}} = P^{-1}$\\ \\
\textbf{Remark} The identity map $Id_V$ is not necessarely the usual identity matrix. Indeed, let $I : \mathbb{R}^2 \xrightarrow{} \mathbb{R}^2 $ 
\begin{center}
    \text{by definition I = }
    $\begin{bmatrix}
    1 & 0 \\
    0 & 1
    \end{bmatrix}$
\end{center}
if we change the basis from $\vec{i},\vec{j}$ to something else, then $Id_V$ changes correspondingly.\\
\section{Composition of Linear Transformations}
 \large$$ V_1 \xrightarrow{T_1} V_2 \xrightarrow{T_2} V_3$$\\
\normalsize
$T_1$ and $T_2$ are linear transformations.\\
Let $\mathcal{B}_i$ be a baiss of $V_i$ where $ i = 1,2,3 $. Then what is the matrix representation of $[T_2 \circ T_1]_{\mathcal{B}_1,\mathcal{B}_3}$ \\
Let $ u \in V_1$
\begin{align*}
    [T_2\circ T_1(u)]_{\mathcal{B}_3} &= [T_2(T_1(u))]_{\mathcal{B}_3}\\
     \large &=[T_2]_{\mathcal{B}_2 , \mathcal{B}_3}[T_1(u)]_{\mathcal{B}_2} \\
    &=[T_2]_{\mathcal{B}_2 , \mathcal{B}_3}[T_1]_{\mathcal{B}_1 ,\mathcal{B}_3}[u]_{\mathcal{B}_1}
\end{align*}
\normalsize
Therefore, 
\begin{center}
    $[T_2\circ T_1]_{\mathcal{B}_1,\mathcal{B}_3} =  \large [T_2]_{\mathcal{B}_2,\mathcal{B}_3}[T_1]_{\mathcal{B}_1,\mathcal{B}_2}$
\end{center}
\normalsize
\textbf{Application}\\
Let $V$ and $W$ be 2 vector spaces such that $T : V \xrightarrow{} W$ is an isomoprhism.\\
Set $n = \dim(V) = \dim(W)$ \\
Let $\mathcal{B}$ and $\mathcal{S}$ be bases of $V$ and $W$ respectively, then \\
\begin{align*}
    &V\xrightarrow{T} W \xrightarrow{T^{-1}} V \\
    &\mathcal{B}\qquad \mathcal{S} \qquad \mathcal{B}
\end{align*}
Using the formula deribved before, 
\begin{align*}
&[Id_V]|_{\mathcal{B} , \mathcal{B}} = [T^{-1}]_{\mathcal{S} , \mathcal{B}}[T]_{\mathcal{B} , \mathcal{S}} \\
&I_n = [T^{-1}]_{\mathcal{S} , \mathcal{B}}[T]_{\mathcal{B} , \mathcal{S}}
\end{align*}
This result is telling us that if $T$ is an isomprhism between $V \xrightarrow{} W$ then $T$ representation is invertible with inverse map.
\begin{Proposition}
	Let $\tvw$ be an isomorphism, $\mathcal{B}, \mathcal{S}$ are two basis of $V$ and $W$ respectively, \newline
	then $[T]_\mathcal{B}{},_\mathcal{S}$ is invertible, moreover its inverse  is :
	\begin{center}
		$([T]_\mathcal{B}{},_\mathcal{S})^{-1} = [T^{-1}]_\mathcal{S}{},_\mathcal{B}$
	\end{center}
	"Therefore it's a matter of invertibility of matrix for isomorphism"
\end{Proposition}
\section{Finding $Ker(T)$ and $Im(T)$ using a matrix representation of $T$}
Let $T : \mathcal{P}_3 \xrightarrow{} \mathcal{P}_2$ and \\
$\mathcal{B} = \{x^2+2x, x^2+2, 1+x, 1-x \}$ and $ \mathcal{S} = \{x^2,x,1\} $ basis of $\mathcal{P}_3$ and $\mathcal{P}_2$ respectively. \\ 
\textbf{Note} $[T]_\mathcal{B}{},_\mathcal{S}$ is a $3\cross 4$ matrix not the opposite !\\ \\
\textbf{Picture: }
\begin{align*}
&\quad\quad \mathcal{P}_3 \text{  }\xrightarrow{T} \mathcal{P}_2\\
&\quad \mathcal{i}_{\mathcal{B}}\big\downarrow\quad\quad\quad \big\downarrow\mathcal{i}_\mathcal{S} \\
&\quad\quad \mathbb{R}^{4} \xrightarrow{\widetilde{T}} \mathbb{R}^{3}
\end{align*}
From the above picture, the kernel of $T$ is the column space of $\mathbb{R}^4$ and its image is the row space in $\mathbb{R}^3$, which are both isomorphic to $\mathcal{P}_3$ and $\mathcal{P}_2$ respectively through $\mathcal{i}_{\mathcal{B}}$ and $\mathcal{i}_{\mathcal{S}}$.\\
\section{Inner Products}
\textbf{Recall} Dot product in $\mathbb{R}^3$ is defined as \\
\begin{equation*}
\vec{u_1} = 
	\begin{bmatrix}
	a_1 \\
	b_1 \\
	c_1
	\end{bmatrix}
\vec{u_2} = 
	\begin{bmatrix}
	a_2 \\
	b_2 \\
	c_2
	\end{bmatrix}
	\qquad \vec{u_1}\cdot{} \vec{u_2}= a_1a_2 + b_1b_2 + c_1c_2
\end{equation*}
\textbf{Propreties}
\begin{align*}
&1. \quad \vec{u_1} \cdot \vec{u_2} = \vec{u_2} \cdot \vec{u_1} \quad \text{(Symmetry)} \\
&2. \quad (\vec{u_1}+\vec{u_2})\cdot \vec{u_3} = \vec{u_1} \cdot \vec{u_3} + \vec{u_2} \cdot \vec{u_3} \quad \text{(Distributivity)} \\
&3. \quad \vec{u} \cdot \vec{u} \geq 0 \quad \text{and} \quad \vec{u} \cdot \vec{u} = 0 \iff \vec{u} = \vec{0} \quad \text{(Dot product is positive definitive)}
\end{align*}
\begin{Definition}
	Let $V$ be a vector space and consider a map from $V\cross V$ into $\mathbb{R}$ which assigns to every pair $(u,v) u \in V, v \in V$ a number denoted $ \la  u,v \ra $ \newline
	The above map is called "inner product" on $V$ if the following proprieties hold.
\end{Definition}
\begin{align*}
&1.\quad  \la  u,v\ra =  \la  v,u\ra \quad \text{(Symmetry)} \\
&2. \quad  \la  \alpha_1 u_1 + \alpha_2 u_2 , v\ra = \alpha_1  \la  u_1,v\ra + \alpha_2 \la  u_2, v\ra \\
&  \quad\forall u_1, u_2 \in V, v\in V , \alpha_1 , \alpha_2 \in \mathbb{R} \\
&3. \quad \forall u \in V , \qquad  \la  u,v\ra \geq 0 \quad \text{and} ,  \la  u,v\ra = 0 \iff u = 0_v
\end{align*}
\textbf{Note} In the above propositions, in number 2 $\alpha$ doesn't get distributed to each element since 
\begin{equation*}
 \la  \alpha u , \alpha v\ra = \alpha \la  u,\alpha v \ra =\alpha^2  \la  u,v \ra \quad \xrightarrow{} bilinearity !
\end{equation*}
\textbf{Example} Let $V = \mathbb{R}^2$. Define $ \la  u,v\ra = u^{T} Av $ , where $A$ is $(2\cross 2)$. \\
Under what conditions is $A$ in the above map an inner product in $\mathbb{R}^2$ ?
\begin{align*}
\text{Proprety 1. (Symmetry)} & \la  u,v\ra =  \la  v,u\ra \forall u,v \in \mathbb{R}^2 \\
	&  \la  u,v\ra \quad \text{is by definition } u^{T}Av \\
	&  \la  v,u\ra \quad \text{is by definition } v^{T}Au \\
	& \xrightarrow{}  \la  u,v\ra \quad \text{is also} = (u^{T}Av)^{T}\\
	& \qquad \qquad \qquad \qquad  \quad = v^{T}A^{T}u 
\end{align*}
\begin{equation*}
\text{So } \quad v^{T}A^{T}u = v^{T}Au \quad \iff A = A^T \quad \text{(Symmetry)}
\end{equation*}
We conclude that we must have $A = A^T$ , i.e $A$ myst be a symmetryc matrix.
\begin{align*}
\text{Proprety 2. } \text{Let} \quad u_1, u_2, v \in \mathbb{R}^2 , \alpha_1, \alpha_2 \in \mathbb{R}\\
 \la  \alpha_1 u_1 + \alpha_2 u_2 ,v\ra &= (\alpha_1 u_1 + \alpha_2 u_2 )^{T}Av \\
& = \alpha_1 u_{1}^{T}Av + \alpha_2 u_{2}^{T}Av \\
& = \alpha_1  \la  u_1, v\ra + \alpha_2  \la  u_2, v \ra 
\end{align*}
In summary, if $A(2\cross 2)$ symmetric matrix, the map $ \la  u,v\ra = u^{T}Av \ra $ satisfies propreties 1 and 2 of the definition of the inner product.
%TODo proprety 3 
\begin{Definition}
	Let A be a $n\cross n$ symmetric matrix. A is said to be positive definite if 
	\begin{align*}
	&1.\forall u \in \mathbb{R}^n , u^{T}Au \geq 0 \quad \text{and} \\
	&2. u^{T}Au =0 \quad \iff u = 0_v
	\end{align*}
\end{Definition}
\begin{Proposition}
	Let $A$ be a positive definite matrix, the formula \newline
	\begin{center}
		$ \la  u,v\ra = u^{T}Av $ defines an inner product in $\mathbb{R}^n$
	\end{center}
\end{Proposition}
\textbf{Example}
\begin{align*}
	&\text{Let } A = 
\begin{bmatrix}
	5 & 1 \\
	1 & 5 
\end{bmatrix}
	\\ 
	&\text{then }\qquad  \la  u,v\ra = u^{T}Av \quad \text{satisfy prop.1 and .2. Question : is A positive definite ?}
\end{align*}
\begin{align*}
	\text{Take }\quad  \la  u,u\ra =u^{T}Au \quad  
	\xrightarrow{so} u = 	
	\begin{bmatrix}
	 x \\
	 y
	\end{bmatrix}\\
	\text{and then , }  \la  u,u\ra =5x^2 + 2xy +5y^2
\end{align*} 
\textbf{Method 1 :}
\begin{align*}
	& \la  u,u\ra =  4(x^2 + y^2) + (x+y)^2 \geq 0\\
	& \la  u,u\ra = 0 \implies x^2 + y^2 = 0 \text{ and} \\
	&\qquad  \qquad \qquad \qquad x+y=0 \\
	&\text{so then } \quad x = y = 0
\end{align*}
Therefore $A$ is positive definite and $ \la  u,v\ra = uA^{T}v $ is an inner product in $\mathbb{R}^2$ \\ \\
\textbf{Method 2 :} Through diagonalization \\
\begin{align*}
	&p_A( \lambda)=( \lambda -4)( \lambda -6) \\
	&A = PDP^{-1} \quad \text{where } D = 
	\begin{bmatrix}
		4 & 0 \\
		0 & 6
	\end{bmatrix} \\
	&P = \begin{bmatrix}
		1 & 1 \\
	   -1 & 1
	\end{bmatrix} \qquad 
	P^{-1} = \frac{1}{2}\begin{bmatrix}
		1 & -1 \\
		1 & 1
	\end{bmatrix}
\end{align*}
\begin{align*}
	 \la  u,u\ra = u^{T}Au = \frac{1}{2} u^{T} 
	\underbrace{\begin{bmatrix}
		1 & 1\\
		-1 & 1
	\end{bmatrix}}_{\text{$Q^{T}$}}
	\begin{bmatrix}
		4 & 0 \\
		0 & 6
	\end{bmatrix}
	\underbrace{\begin{bmatrix}
		1 & -1 \\
		1 & 1
	\end{bmatrix}}_{\text{$Q$}} u 
\end{align*}
\begin{align*}
	\text{Set } \quad u = 
	\begin{bmatrix}
		x \\
		y
	\end{bmatrix}
	\qquad &\text{ then  }  \la  u,u\ra = u^{T}Au \\
	& = \frac{1}{2}u^{T} 
	\begin{bmatrix}
			1 & -1\\
			1 & 1
			\end{bmatrix}
	\begin{bmatrix}
	1 & -1 \\
	1 & 1
	\end{bmatrix}
	\begin{bmatrix}
	x \\ 
	y
	\end{bmatrix}\\
	&\xrightarrow{} = \frac{1}{2}(Qu)^{T}
	\begin{bmatrix}
	4 & 0 \\
	0 & 6
	\end{bmatrix} (Qu) \\
	\xrightarrow{} Qu = 
	\begin{bmatrix}
		x-y \\
		x+y
	\end{bmatrix} \quad \text{So that }\\
	&  \la  u,u\ra = \frac{1}{2}(4(x-y)^2 + 6(x+y)^2)
\end{align*}
So in that new coordinate system it's 4 times the first coordinate and 6 times the second coordinate of $P$ ,indeed the column vector of the matrix $P$ defined in the above example are two new basis vectors. \\ \\ 
\textbf{Facts } If $A$ is a $n\cross n$ symmetric matrix then,
\begin{align*}
&\textbf{1. } \text{A is diagonalizable} \\
&\textbf{2. }A = PDP^{-1}\\
&\qquad \text{then matrix P can be chosen such that}
\end{align*}
\begin{gather*}
P^{-1} = P^{T} \\
\text{so that } A = PDP^{T} \xrightarrow{} \text{ orthogonal diagonalization}
\end{gather*} 
\textbf{Remark :} Given $2\cross 2$ symmetric matrix, when is $A$ positive definite ? 
\begin{center}
	When eigenvalues are larger than 0.
\end{center}
\begin{align*}
	&A =PDP^{T} , \qquad \text{set} \quad D =
	\begin{bmatrix}
		 \lambda_1 & 0 \\
		0 		  &  \lambda_2
	\end{bmatrix} \\
	& P = 
	\begin{bmatrix}
	 \underbrace{u_1}_\text{$2\cross 1$} & \underbrace{u_2}_\text{$2\cross 1$}
	\end{bmatrix}
\end{align*}
\begin{Proposition}
	In general, a symmegric $n\cross n$ matrix $A$ is positive definite if and only if all eigenvalues of $A$ are positive
\end{Proposition}
\section{Examples of Inner Products}
\textbf{Example } Let $V = \mathcal{P}_n$ 
\begin{gather*}
\text{set }  \la  f,g\ra = \int_{0}^{1} f(t)g(t)dt  
\end{gather*}
\begin{align*}
	&\textbf{1. }  \la  f,g\ra =  \la  g,f\ra \qquad \text{Symmetry proprety holds} \\
	&\textbf{2. }  \la  \alpha_1 f_1 + \alpha_2 f_2 , g\ra \\
	&\qquad \qquad \int_{0}^{1} (\alpha_1 f_1 (t) +\alpha_2 f_2 (t))g(t)dt \\
	&\qquad \qquad =\alpha_1 \int_{0}^{1} f_1 (t) g(t) dt + \alpha_2 \int_{0}^{1} f_2 (t)g(t)dt \\
	& \qquad \qquad =\alpha_1  \la  f_1 ,g\ra + \alpha_2  \la  f_2 ,g\ra \quad \xrightarrow{} \text{ holds!} \\
	&\textbf{3. }  \la  f,f\ra = \\
	& \qquad \int_{0}^{1} (f(t))^{2} dt \geq 0 \qquad \text{ by propreties of integrals} \\
	& \text{Now when is this equal to zero ? } \\
	&\quad \int_{0}^{1}(f(t))^{2} dt = 0 \iff f(t) = 0 \quad \forall t \in [0,1] 
\end{align*} \\ 
\textbf{Note about previous example } The integral is 0 on $[0,1]$ which implies that all of this function on $\mathbb{R}^{n}$ is equal to zero as well since we're in $\mathcal{P}_n$ which has at most $n$ roots , but on interval $[0,1]$ there are $\infty$ roots ! \\ \\
\textbf{Example } Let $V = \mathscr{M}_{2\cross 2}$ where $ \la  A,B\ra = \tr(AB^{T})$
\begin{align*}
	&\textbf{1. }  \la  A,B\ra = \tr(AB^{T}) \\
	& \qquad \tr(AB^{T})^{T} = \tr(BA^{T}) \\
	& \hspace{67pt} =  \la  B,A\ra \\
	& \textbf{2. }  \la  \alpha_1 A_1 + \alpha_2 A_2, B\ra  \\
	&\hspace{67pt} = \tr((\alpha_1 A_1 + \alpha_2 A_2)B^{T}) \\
	&\hspace{67pt} = \tr(\alpha_1 A_1 B^{T}) + \tr(\alpha_2 A_2 B^{T}) \\
	&\hspace{67pt} =\alpha_1  \la  A_1 , B\ra + \alpha_2 \la  A_2 , B  \ra \\ 
	&\textbf{3. } \text{Set } A = (a_{ij}) , \quad  \la  A,A\ra = \tr(AA^{T}) = \sum_{i,j}^{} a_{ij} \geq 0 \\
	&\quad \text{so }  \la  A,A\ra = 0 \xrightarrow{} a_{ij} = 0 \quad \forall i,j\\
	&\hspace{90pt} A = \underbrace{\utilde{O}}_\text{$n\cross n$}
\end{align*}
\begin{Definition}{Norm of a vector} \\
	We will generalize the definition of norm of a vector in $\mathbb{R}^3$ which is $\sqrt{x_{1}^2 + x_{2}^2 +x_{3}^2}$. Since this is true in $\mathbb{R}, \mathbb{R}^2$ \\ 
	Let V be equipped with an inner product. The norm of $u (u\in V)$ is denoted $\norm{u}$ and defined as 
	\begin{gather*}
		\norm{u} = \sqrt{ \la  u,u \ra } 
	\end{gather*}
\end{Definition}
\ex
\begin{gather*}
	V  = \mathcal{P}_2 \qquad  \la  f,g\ra = \int_{0}^{1} f(t) g(t) \ dt \\
	p(t) = t \qquad \implies \norm{p} = \frac{\sqrt{3}}{3}
\end{gather*}
\begin{Definition}{Orthogonal vectors} \\
	Let $V$ be a vector space equipped with an inner product $ \la  , \ra $ \\
	Two vectors $u,v \in V$ are said to be orthogonal if \\
	\begin{gather*}
		 \la  u,v\ra = 0
	\end{gather*}
\end{Definition}
\textbf{Note} Orthogonality is implicitly with respect to a dot product \\ \\ 
\textbf{Propreties} 
\begin{align*}
	&\textbf{1. Cauchy-Schwarz Inequality} \\
	& \abs{ \la  u,v \ra } \leq \sqrt{ \la  u,u \ra } \sqrt{ \la  v,v \ra } \\
	&\qquad \text{this equality holds iff $u$ and $v$ are $//$. i.e} \\
	& \qquad u = \alpha v \text{ or} \\
	& \qquad v = \beta u \\
	& \textbf{2. Pythagorean Equality} \\
	& \text{if } u \perp v \iff  \la  u,v\ra = 0  \quad \text{then}  \\
	& \hspace{40pt} \norm{u+v}^2 = \norm{u}^2 + \norm{v}^2 \\
	& \text{else } \\
	& \hspace{40pt} \norm{u+v}^2 = \norm{u}^2 + \norm{v}^2 + 2 \la  u,v\ra \\
\end{align*}
\textbf{Note} 
For the previous proprieties, note that the discriminant in the quadratic formula is basically Cauchy-Schwarz's Inequality ,because if quadratic function is larger than 0 then there are no roots.
\begin{Definition}
	Let $V$ be a vector space equipped with an inner product.
	\begin{enumerate}
	\item A set $\mathcal{S} = {\un}$ is called an orthogonal set if $ \la  u_i, u_j\ra = 0$ whenever $ i \neq j$
	\item A proper orthogonal set is an orthogonal set which does not contain the zero vector $0_V$
	\item An orthonormal set is an orthogonal set $\mathcal{S} = {\un}$ such that 
	\begin{gather*}
		\norm{u_i} = 1 \qquad \forall i = 1,2 \dots ,n \\
		 \la  u_i, u_i\ra = 1 \qquad \forall i = 1,2 \dots, n
	\end{gather*}
	\end{enumerate}
\end{Definition}
\ex 
$V = \mathcal{R}^3$, equipped with an inner product. 
\begin{gather*}
	\mathcal{S}_1 = {
	\begin{bmatrix}
		1 \\
		1 \\
		-1
	\end{bmatrix}
	,
	\begin{bmatrix}
		2 \\
		1 \\
		0
	\end{bmatrix}
	} \quad \text{is not an orthogonal set , therefore not orthonormal as well}
	\intertext{Find $x$ such that it's an orthogonal set} 
	\mathcal{S}_2 = 
	\begin{bmatrix}
		1 \\
		1 \\
		-1 
	\end{bmatrix}
	,
	\begin{bmatrix}
		2 \\
		x \\
		0
	\end{bmatrix}
	\qquad x =-2 \quad \text{proper orthogonal set, but not orthonormal} \\
	\intertext{Turn this into an orthonormal set}
	\mathcal{S}_3 = {
	\frac{1}{\sqrt{3}}
	\begin{bmatrix}
		1 \\
		1 \\
		-1
	\end{bmatrix}
	, \frac{1}{\sqrt{2}}
	\begin{bmatrix}
		1 \\
		-1 \\
		0
	\end{bmatrix}
	} \qquad \text{this is an orthonormal set}
\end{gather*}
\textbf{Note} An orthonormal set is a proper orthogonal set. Every orthonormal vector should have length one, but length of zero vector is zero.
\begin{Proposition}
	$V$ is a vector space equipped with an inner product. Then every finite proper orthogonal set of $V$ is linearly independent.
\end{Proposition}
\textbf{Note last proposition} "You can increase dimension only when you have a bigger orthogonal set" \\
\begin{corollary}
	Assume $\dim(V) = n$ 
	\begin{enumerate}
	\item Every proper orthogonal set with $n$ vectors is a basis of $V$ called an orthogonal basis of $V$. 
	\item Every orthonormal set of $V$ with $n$ vectors is a basis of $V$ called orthonormal basis.
	\end{enumerate}
\end{corollary}
\textbf{Note} $\{x^2,x,1\} \implies x \perp 1 , x \perp x^2 $ but $ x^2$ is not perpendicular to $1$. \\
\begin{Proposition}
	Let $V$ be a vbector space equipped with an inner product. \\
	Let $E$ be a subspace of $V$ such that $\dim(E) =n$. \\
	Let $\mathcal{B} = {\un}$ be an orthogonal basis of $E$.
	\begin{enumerate}
	\item (Bersels Inequality) 
	\begin{gather*}
		\forall u \in V, \quad \text{the vector } \\
		w = u - \sum_{i=1}^{n}\frac{ \la  u,u_i \ra }{ \la  u_i,u_i \ra } u_i \quad \text{is orthogonal to every vector in $E$} \\
		\text{Moreover,} \quad \norm{u}^2 \geq  \sum_{i=1}^{n}\frac{( \la  u,u_i \ra )^2}{ \la  u_i,u_i \ra }
	\end{gather*}
	\item (Fourier's Indentity) 
	\begin{gather*}
		\text{Whenever } u \in E \ \text{then,} \\
		u = \sum_{i=1}^{n}\frac{ \la  u,u_i \ra }{ \la  u_i,u_i \ra } u_i 
	\end{gather*}
	\item (Perseval Equality)
	\begin{gather*}
	\text{Whenever } u \in E \ \text{then,} \\
	\norm{u}^2 = \sum_{i=1}^{n}\frac{( \la  u,u_i \ra )^2}{ \la  u_i,u_i \ra }
	\end{gather*}
	\end{enumerate}
\end{Proposition}
\textbf{Remark} Given $u \in V \ \exists ! \ v\in E$ such that $w = u-v \perp E$.
\begin{Definition}
	Let $E$ be a finite dimensional subspace of a vector space $V$ (which is equipped with an inner product). Let $u \in V$ \\
	The unique vector $v \in E$ such that $w = u -v \in E $ is called the orthogonal projection of $u$ onto $E$, and is denoted  $\vectorproj[E]{u}$. \\
	Moreoever, if ${\un}$ is an orthogonal basis of $E$ then ,
	\begin{equation*}
		\vectorproj[E]{u} = \sum_{i=1}^{n} \frac{ \la  u,u_i \ra }{ \la  u_i,u_i \ra } u_i
	\end{equation*}	
\end{Definition}
\textbf{Note about polynominal projections} If we're asked to project $\mathcal{P}_2$ onto $\mathcal{P}_3$ then since $\mathcal{P}_2 \in \mathcal{P}_3$ the projection is itself. \\

\begin{Proposition}
	Finding a $\perp$ basis. Gram=Schmidt Algorithm. \\
	$\dim(E) = n$ , $E$ is a subspace of a vector space $V$ *that is equipped with an inner product). \\
	Let $\mathcal{B} = \{v_1, v_2, \dots, v_n\}$ an arbitrary basis of $E$ (so not necessarely orthogonal) \\
	Set 
	\begin{align*}
	 E_1 &= \text{Span}\{v_1\} \\
	 E_2 &= \text{Span}\{v_1,v_2\} \\
	 \vdots \\
	 E_k &= \text{Span}\{v_1,v_2,\dots,v_k\}
	\end{align*}
	\textbf{Note} $E_1 \subseteq E_2 \subseteq E_3 \dots \subseteq E_n$ $\xrightarrow{}$ Nested subspaces. \\
	Define 
	\begin{align*}
	\longboxed{
		 u_1 &= v_1 \\
	 	u_2 &= v_2 - \vectorproj[E_1]{v_2} \\
		 u_3 &= v_3 - \vectorproj[E_2]{v_3} \\
	 	\vdots \\
	 	u_k &= v_k - \vectorproj[E_{k-1}]{v_k} }
	\end{align*}
	And so we just jumped from an arbitrary basis to a $\perp$ basis, indeed $u_1 \perp u_2 , u_3 \perp (u_1 \ \text{and} \ u_2)$
	Finally, 
	\begin{equation*}
		E_k = \text{Span}\{u_1,u_2, \dots, u_k\} \quad \text{where} \quad \{u1,\dots,u_k\} \ \text{is a basis of } E_k
	\end{equation*}
\end{Proposition}
\textbf{Example } \\
$V = \mathcal{P}_2$, $\mathcal{B} = \{p_1, p_2, p_3\}$ the canonical basis of $\mathcal{P}_2$.
Define the usual inner product 
\begin{equation*}
	 \la  f,g\ra = \int_{-1}^{1} f(t) g(t) \ dt
\end{equation*}
Find an orthogonal basis.\\
Let us use the Gram-Schmidt algorithm to find a $\perp$ basis. 
\begin{align*}
	q_1 &= p_1 \\
	q_2 &= p_2 - \frac{ \la  p_2, p_1 \ra }{ \la  p_1, p_1 \ra }p_1 \xrightarrow{} q_2(x) = x-0 = x \\
	q_3 &= p_3 - \frac{ \la  p_3,q_1 \ra }{ \la  q_1,q_1 \ra }q_1 - \frac{ \la  p_3,q_2 \ra }{ \la  q_2, q_2 \ra } q_2 
\end{align*}
\begin{gather*}
	\text{replacing the values in the given integral yields :} \\
	 q_3(x) = x^2 -  	\frac{\sfrac{2}{3}}{2}(1) - 0 = x^2 - \sfrac{1}{3} \\
	\intertext{Thus, } \{q_1,q_2,q_3 \} \quad \text{is an orthogonal basis of } \mathcal{P}_2
\end{gather*}
\section{Orthogonal Sets} 
Let $V$ be a vector space equipped with an inner product. Let $\mathcal{S}$ be a subset of $V$. 
\begin{Definition}
	The orthogonal set to $\mathcal{S}$ denoted 
	\begin{equation*}
	 \mathcal{S}^{\perp} = \{v \in V |  \la  u,v\ra = 0 \ \forall u \ \in \ \mathcal{S} \}
	\end{equation*}
\end{Definition}
\textbf{Example} \\
What is orthogonal to $0_V$ and $V$ ? 
\begin{align*}
	&\{0_V\}^{\perp} = V \\
	&\{V\}^{\perp} = \{0_V\}
\end{align*}
\textbf{Note} \\
The bigger the set, the smaller the orthogonal compliment
\begin{Proposition} 
	Propreties of orthogonal sets.
	\begin{enumerate}
		\item $\mathcal{S}^{\perp} $ is a subspace of $V$ 
		\item Assume $\mathcal{S} $ is finite (can be infinite as well) ,then 
	\end{enumerate}
	\begin{equation*}
		\mathcal{S}^{\perp} = (\text{Span}\{\mathcal{S}\})^{\perp}
	\end{equation*}
	So it's convenient to check for spanning set.
\end{Proposition}
\begin{Proposition}
	Let $V$ be a vector space equipped with an inner product. Let $E$ be a subspace of $V$ such that $\dim(E) = n$, then 
	\begin{enumerate}
	\item $E \oplus E^{\perp} = V$ 
	\item $(E^{\perp})^{\perp} = E $ 
	\end{enumerate}
\end{Proposition}
For that reason , $E^{\perp}$ is called the orthogonal compliment of $E$. \\
\\
\textbf{Important Example}\\
$V = \mathcal{P}_2 ,\quad \mathcal{B} = \{x^2, x, 1\} = \{p_1, p_2, p_3\}$
\begin{gather*}
	\la f,g \ra = \int_{-1}^{1} f(t) g(t) \ dt \quad \text{Canonical isomorphism induced by $V$}\\
	\mathcal{i}_{\mathcal{B}}: V \xrightarrow{} \mathbb{R}^3 \ \ \forall \ x ,y \ \in \ \mathbb{R}^3 , \mathcal{i}_{\mathcal{B}}^{-1} (x) ,\mathcal{i}_{\mathcal{B}}^{-1} (y) \\
	\text{Ex : } \quad \utilde{x} = 
		\begin{bmatrix}
			2 \\
			1 \\
			0
		\end{bmatrix}, \quad \mathcal{i}_{\mathcal{B}}^{-1} (x)  = p = 2x^2 + x \quad \text{"Rewriting polynominal"} \\
		\text{ go from $\mathbb{R}^3$ into the set of polynominals}\\
		\intertext{Claim :} \\
		\la x,y \ra_1 - \la  \mathcal{i}_{\mathcal{B}}^{-1} (x) ,  \mathcal{i}_{\mathcal{B}}^{-1} (y) \ra \quad \text{is an inner product of $\mathbb{R}^3$}\\
		\intertext{We have to prove all propreties : }
\end{gather*}
\begin{align*}
		&\text{1. } \ \ \la x,y \ra = \la  \mathcal{i}_{\mathcal{B}}^{-1} (x) ,  \mathcal{i}_{\mathcal{B}}^{-1} (y) \ra \quad \text{both are } \in V \quad \text{so we can use symmetry} \\
		&\hspace{20pt}\la x,y \ra = \la  \mathcal{i}_{\mathcal{B}}^{-1} (y) ,  \mathcal{i}_{\mathcal{B}}^{-1} (x) \ra  = \la y,x \ra. \\
		&\text {2. } \text{Let } x,y_1, y_2 \ \in \mathbb{R}^3 \ , \alpha_1, \alpha_2 \ \in \mathbb{R} \\
		&\hspace{20pt} \la x, \alpha_1 y_1 + \alpha_2 y_2 \ra = \la  \mathcal{i}_{\mathcal{B}}^{-1} (x) ,  \mathcal{i}_{\mathcal{B}}^{-1} (\alpha_1 yu_1 + \alpha_2 y_2) \ra \\
		&\hspace{101pt}= \la  \mathcal{i}_{\mathcal{B}}^{-1} (x), \alpha_1  \mathcal{i}_{\mathcal{B}}^{-1} (y_1) + \alpha_2  \mathcal{i}_{\mathcal{B}}^{-1} (y_2) \ra \\
		&\hspace{101pt} = \alpha_1 \la  \mathcal{i}_{\mathcal{B}}^{-1} (x),  \mathcal{i}_{\mathcal{B}}^{-1} (y_1) \ra + \alpha_2 \la  \mathcal{i}_{\mathcal{B}}^{-1} (x) ,  \mathcal{i}_{\mathcal{B}}^{-1} (y_2) \ra \\
		&\hspace{101pt} = \alpha_1 \la x, y_1 \ra + \alpha_2 \la x, y_2 \ra.\\
		&\text{3. } \text{Let } x \ \in \mathbb{R}^3 \\
		&\hspace{101pt} \la x,x \ra_1 = \la  \mathcal{i}_{\mathcal{B}}^{-1} (x) ,  \mathcal{i}_{\mathcal{B}}^{-1} (x) \ra \geq 0\\
		& \hspace{20pt} \text{And , } \la x,x \ra_1 = 0 \iff  \mathcal{i}_{\mathcal{B}}^{-1} (x) = 0 \implies x = 0 \\
		\intertext{ So it is an inner product, the inner product is defined as : } \\
		&\hspace{20pt} \la x ,y \ra = x^{T} A y \quad \text{For some positive definite matrix $A$, but what is $A$ ?} \\
		&\hspace{50pt}\la  \mathcal{i}_{\mathcal{B}}^{-1} (x),  \mathcal{i}_{\mathcal{B}}^{-1} (y) \ra = x^{T} A y \\
		&\hspace{20pt}\text{If } \ p,q \in \ \mathcal{P}_2 , \quad \text{set } \ x = [p]_{\mathcal{B}}, y = [q]_{\mathcal{B}} \ \text{the coordinate vectors} \\
		&\hspace{50pt}\implies  \la p,q \ra = [p]_{\mathcal{B}}^{T} A [q]_{\mathcal{B}}\\
		\intertext{The inner product on $\mathbb{R}^3$ can be represented through coordinate system. Therefore, we can find the transition matrix this way !}
		&\text{Our basis is} \ \mathcal{B} = \{p_1, p_2, p_3 \} \\
		&\hspace{50pt} \implies [p]_{\mathcal{B}}^{T} A [q]_{\mathcal{B}} = \int_{-1}^{1} p_i (t) p_j (t) \\
		\intertext{So the matrix $A$ , after computation is :} 
\end{align*}
\begin{gather*}
		A = 
		\begin{bmatrix}
			\sfrac{2}{5} & 0 & \sfrac{2}{3} \\
			0 & \sfrac{2}{3} & 0 \\
			\sfrac{2}{3} & 0 & 2
		\end{bmatrix} \quad 
		\text{For instance , $a_{11}$ was found by doing } \\
		a_{11} = \int_{-1}^{1} x^2 x^2 \ dx = \dfrac{2}{5}
\end{gather*}
\\
\textbf{Note }\\
If we're given $ T : \mathbb{R}^{n} \xrightarrow{} \mathbb{R} $ and suppose $T \neq 0 $ , can we say something about Ker(T) ?\\
\begin{gather*}
	\dim(\text{Ker}T) = n -1 \qquad \text{because $\dim(\text{Im}(T))=1$ , by Rank-N-theorem} 
\end{gather*} 
\\
\textbf{Note} \\
\begin{enumerate}
	\item Orthogonality implies linear independence ,but linear independence doesn't imply orthogonality. 
	\item If $A$ is positive definite then $A$ is a symmetric matrix. If $A$ is a symmetric matrix , then $A$ is not necessarely positive definite.
\end{enumerate}


\end{document}
